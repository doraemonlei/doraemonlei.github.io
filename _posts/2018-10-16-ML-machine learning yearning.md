---
layout: post
title: "机器学习：Machine Learning Yearning"
subtitle: " \"Hello World, Hello Work\""
date: 2018-10-17 15:33:16
author: "doraemonlei"
#header-img: "img/post-bg-2015.jpg"
#cover: 'http://on2171g4d.bkt.clouddn.com/jekyll-theme-h2o-postcover.jpg'
categories: test
tags: 机器学习 深度学习
---

 <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

> 吴恩达新书《Machine Learning Yearning-Technical Strategy for AI Engineers, in the Eera of Deep Learning》，主要介绍机器学习实际使用时的一些策略和技巧，以便为开发指明方向，提升开发效率。

# 目录

1. **[本书概述]()**
    - **[1.为什么要说机器学习策略](#1)**
    - **[2.本书对你和你的团队有哪些帮助](#2)**
    - **[3.阅读本书的先决条件](#3)**
    - **[4.驱动机器学习长足进步的因素](#4)**
2. **[构建开发和测试数据集]()**
    - **[5.开发和测试数据集的重要性](#5)**
    - **[6.使用同一分布的开发和测试数据集](#6)**
    - **[7.开发和测试数据集多大合适](#7)**
    - **[8.使用单值衡量指标](#8)**
    - **[9.优化性指标和约束性指标](#9)**
    - **[10.在开发/测试数据集和单值衡量指标的基础上，加速模型迭代](#10)**
    - **[11.何时需要改变数据集和衡量指标](#11)**
    - **[12.构建开发和测试数据集的注意事项](#12)**
3. **[基本的误差分析]()**
    - **[13.快速构建你的系统，然后迭代](#13)**
    - **[14.误差分析的重要性和大致过程](#14)**
    - **[15.并行评估多个想法的可行性](#15)**
    - **[16.清除标记错误的样本数据](#16)**
    - **[17.将开发数据集分成人工观测数据集和模型调参数据集](#17)**
    - **[18.人工观测数据集和模型调参数据集多大合适](#18)**
    - **[19.总结：基本误差分析构建开发和测试数据集](#19)**
4. **[偏差和方差]()**
    - **[20.偏差和方差的概念](#20)**
    - **[21.举例说明偏差和方差](#21)**
    - **[22.向最优的错误率看齐](#22)**
    - **[23.方差和偏差的处理方法](#23)**
    - **[24.权衡模型的方差和偏差](#24)**
    - **[25.减少可避免的偏差方法](#25)**
    - **[26.训练数据集上的误差分析](#26)**
    - **[27.减少方差的方法构建开发和测试数据集](#27)**
5. **[学习曲线]()**
    - **[28.通过学习曲线诊断偏差和方差](#28)**
    - **[29.将训练错误率用图形绘制出来](#29)**
    - **[30.高偏差时的学习曲线](#30)**
    - **[31.其他情况下的学习曲线](#31)**
    - **[32.学习曲线绘制技巧构建开发和测试数据集](#32)**
6. **[与人类的水平进行比较]()**
    - **[33.为社么要与人类的水平进行比较](#33)**
    - **[34.如何定义人类处于什么样的水平](#34)**
    - **[35.超越人类的水平](#35)**
7. **[训练和测试数据集的分布不一致]()**
    - **[36.何时使用不同分布的数据集](#36)**
    - **[37.是否要将你的所有数据都用起来呢](#37)**
    - **[38.是否要使用不一致的数据](#38)**
    - **[39.区分不同数据的权重](#39)**
    - **[40.从训练数据集泛化到开发数据集](#40)**
    - **[41.区分偏差、方差和数据不匹配问题](#41)**
    - **[42.处理数据不匹配问题](#42)**
    - **[43.人工数据合成](#43)**
8. **[算法调试]()**
    - **[44.优化验证实验](#44)**
    - **[45.优化验证实现的一般形式](#45)**
    - **[46.强化学习实例](#46)**
9. **[端到端深度学习]()**
    - **[47.端到端学习的兴起](#47)**
    - **[48.更多的端到端学习实例](#48)**
    - **[49.端到端学习的利与弊](#49)**
    - **[50.选择管道组件之数据可用性](#50)**
    - **[51.选择管道组件之任务简单化](#51)**
    - **[52.让机器学习输出更加丰富的内容](#52)**

> 注：可点击**标题**直接跳转

# <span id='33'>33.为什么要与人类的水平进行比较 ?</span>
机器学习系统的目标是让机器自动化的完成人类能做的很好的事情。比如图片识别、语音识别、垃圾邮件分类。学习算法已获得的长足进步，在很多任务上表现出的性能超过了人类。   

在人类能做的很好的任务中，构建机器学系统相对容易一些，原因有以下几点：   

1. 通过人工标注很容易获取数据。比如，因为人可以很好的识别猫类图片，你的算法可以获得人类提供的高精度标记数据。 

2. 人类可以凭直觉分析误差。比如语音识别算法的性能低于人类识别水平，比如一段音频被错误的识别成“This recipe calls for a pear of apples"，错误把"pair"识别成"pear"了。你可以凭直觉想想，人类怎么来修正这些错误，并把这些知识来修正你的算法。 

3. 可以用人类水平来评估算法的最优错误率，并设置期望的错误率。比如，你的算法在某项任务上的错误率为10%，而在这项任务上，人类能达到2%的错误率，那我们就知道最优的错误率为2%或者更低，因此可以避免的偏差最少为8%。此时你可以试试减少偏差的相关技术。 

即使第三项看上去，可能并不那么重要，但是设置一个合理的且能达到的目标，的确可以帮助一个团队，加速他们的进度。知道你的算法有很高的偏差是非常价值的，这时我们就可以有方向的选择一些工作去做了。   

有些工作可能人类也不擅长。比如向你推荐一本书、或向网站用户推荐一条广告、或预测股票行情。计算机在这些工作上，已经超出了大部分人的表现。在这些应用中，我们会遇到以下问题：   

1. 很难获取样本标签。人类很难标记出给用户推荐那些书最好。如果你运营一个售书的网站或APP，你可以通过展示书籍或查看用户以前的购买书籍，来获取这些数据。如果没有运营这样的网站，那么你就得想着其他法子，来获取这些数据。 

2. 人类的直觉很难靠得住，比如没人能准确的预测股票的行情，因此如果股票行情预测算法还没瞎猜的好，我们也很难找到解决的办法。 

3. 很难知道最优的错误率和合理的期望值。比如你的图书推荐系统已经做得相当好了，但是你怎么知道相对人来说，还有多大的提升空间呢？

# <span id='34'>34.如何定义人类处于什么样的水平 ?</span>
假设你正在开发一个医学成像应用，它能自动地从X射线图像中做出诊断。一个没有医学背景人，通过简单训练后，能达到15%的错误率，初级医生能达到10%的错误率，一个老练医生可以达到5%的错误率。一个小型的医生小组通过讨论和辩论后，能达到2%的错误率。那我们要把哪一个定义为人类水平呢？ 

在这种情况下，我们将用2%的错误率作为我们最优的错误率。你也可以将2%错误率作为算法的期望值，如上一章所说，这么做有三个好处： 
1. 容易获取人类标记的数据。你可以找一个医生团队，来标记样本标签，这样你就能获得2%错误率的样本集了。 
2. 可以凭直觉进行错误分析。你可以利用医生的直觉，来帮助你改进模型。 
3. 用人类水平来评估最优的错误率并设置模型的期望值。用2%错误率来作为我们的目标是合理的。最优的错误率应该低于2%，不会更高，因为医生团队可以达到2%的错误率。 

当需要标注数据时，往往不需要医生团队针对每张图片都讨论后给出标记，因为他们的时间是非常昂贵的。或许你可以找一个初级医生标记大多数的样本，然后将较难识别的样本交给成本较高的医生或医生团队。 

如果你的系统当前的错误率为40%，那么使用初级医生或者老练医生来标记数据并没多大区别。但是，如果你的系统错误率已经降到10%了，那么将人工水平设置到2%，会非常有助于你改进系统。

# <span id='35'>35.超越人类的水平 ?</span>
假如你正在做一款语音识别产品，现在你收集了很多语音数据。假如你的语音数据中有很多噪音，因此即使是人类也会有10%的识别错误率。假如你的系统识别错误率已经降到了8%，那你还能用第33章所说的三项技术来改进你的系统吗？ 

如果你可以定义一个数据子集，在这个子集上人类的水平显著高于你的系统，那么你仍然可以用之前所说的技术来推动你的系统快速发展。比如，假设你的系统在含噪音的音频中，识别准确率高于人类，但人类在非常快速的口语识别中，依然更为优秀。 

对于口语非常快的数据子集： 
1. 你可以找人帮你对快速口语音频进行标记，从而获取高精度的样本数据。 
2. 你可以通过人类的直觉来理解为什么人类可以正确的识别快速口语，但是你的算法却不行 
3. 你可以将人类在快速口语上的水平，作为你算法预期的目标。 

更一般的来说，即使你的算法的平均性能已经超过了人类的水平，只要能找到人类能做对，但是机器做不对的样本集，那么之前所说的技术都可以使用。 

许多重要的机器学习应用已经超过了人类水平，比如预测电影收视率、火车的送货时长、是否要批准贷款。还有一种情况是人类分类很难，机器也不能处理的很好。因此当机器已经超过人类水平的时候，算法的进展就会变慢了，当机器还在追赶人类时，算法的进展就会快的多。

# <span id='36'>36.何时使用不同分布的数据集 ?</span>
你的猫咪图片应用中，用户已经上传了10,000张图片，你手工标注出哪些图片中含有猫，那些图片中没有猫。此外你还有一个更大的从网上下载而来的数据集，这个数据集包含了200,000张图片。你应该如果定义训练/开发/测试样本集呢？ 

由于用户上传的10,000张图片，反应了真实数据的概率分布，你可能会将其用作开发/测试样本集。如果你训练的模型需要大量的数据，你可能会使用从互联网上采集的200,000图片用作训练。现在，你的训练样本集和开发/测试样本集的概率分布是不同的。这会对你的工作造成什么影响呢? 

在没有分割数据之前，我们总共有210,000张图片，我们可以将他们随机打乱，并放入到训练/开发/测试样本集中。这样所有的数据来自同一分布了。但我并不推荐这么做，因为这么做的话开发/训练样本集中会有200,000/210,000 = 95%来自互联网的图片，这些图片并不能反映实际的数据分布。记住我们所说的选择开发/测试样本集的建议：  
<font color='red'>**开发/测试样本集中尽量放入能反映将来真实数据的样本**</font>

机器学习学术文献中，大多假设训练/开发/测试样本集服从统一分布。在机器学习的早期，数据是比较缺乏的。我们通常只从一个概率分布中采集数据，因此我们随机将数据分割成训练/开发/测试样本集，这种情况，假设所有数据来自同一分布是成立的。 

但是在大数据时代，我们可以访问大量的数据，比如可以从互联网上下载大量的猫类图片。尽管训练数据集相对开发/测试数据集来自不同的分布，但是我们仍然会在机器学习中使用它们，因为它可给我们提供很多的信息。 

针对猫咪检测例子，我们会放置5000张用户上传的图片到开发/测试集中，而不是直接将用户上传的10,000图片到开发/测试集中。我们会把用户上传剩余的5000张图片放到训练样本集中，这样训练样本集中就包含了5000张来自用户上传的数据和200,000张从互联网上下载来的数据，共计205,000张图片。我们会在后续的章节中说明，为什么这么做有效。 

我们再想想第二个例子，假如你要做一个语音识系统，将街道地址转录成文本，用于语音控制地图或导航应用。你有20,000用户的所说的街道样本。但是你还有500,000个其他样本，这些样本中，用户再讨论其他话题。你可以将10,000个街道语音样本放入开发/测试集中，将剩余的10,000个样本和其他的500,000个样本放入训练样本集中。 

我们继续假设开发样本集中的数据和测试样本集中的数据来自同一分布。但是理解训练样本集和开发/测试样本分布不一致非常重要，因为这会给你带来一些特殊的挑战。

# <span id='37'>37.是否要将所有数据都用作训练 ?</span>
假如你的猫咪检测器训练样本集中包含10,000张用户上传的图片。这些数据来自同一分布，可以把它们用作开发/测试样本集上。你还有额外的20,000张图片，这些图片是从互联网上下载的，应该要把20,000 + 10,000 = 30,000张图片一起放到训练样本集中，用作训练吗？ 

在早期的机器学习算法中(手工设计的计算机视觉特征，然后用一个简单的线性模型进行分类)，把数据合并到一起，的确可能会让你的算法变得更糟。因此有些工程会警告你，不要把这20,000张图片放入训练集中。 

但是在当今强大，灵活的机器学习算法中(比如大型的神经网络)，这些风险已经大大的降低了。你可以用大量的神经元或隐藏层来构建神经网络，你可以放心的将20,000张图片放到训练样本集中。增加图片后，算法的性能更可能会获得提升。这种观察基于两种数据类型都有x->y映射关系的事实。也就是说存在一些系统，输入来自互联网或来自用户的图片，都能可靠的预测出样本标签，甚至不知道来源的图片，也能识别的很好。 

将额外的20,000张图片加入训练样本集，会产生如下的效果： 它为你的神经网络提供了更多像猫或不像猫的样本，这对模型来说非常有帮助，因为互联网图片和用户上传的图片，有着一些共同点。你的神经网络可以从网络图片中学习到一些知识，然后应用于用户上传的图片中。 它迫使你的神经网络增加更多的功能，以便学习互联网图片的特殊特征(比如高分辨率、不同分布的图形)，如果这些特征与APP上传的图片特征差别很大，则他可能会耗尽神经网络的能力，因此用于手机APP图片识别的能力就更少了，这是你需要真正关心的。理论上，这并不会对你的算法性能(准确率)有何坏处。 

我们可以用另外一种方式描述第二项，我们引用虚构人物福尔摩斯的话：你的大脑像个楼阁，因此它的空间是有限的，他还说：“当增加知识时，你就会忘记一些其他的事情。因此一定要记住，不要用无用的知识把有用的知识挤出大脑”。 

幸运的是，如果你有足够的计算能力，那么你就可以构建足够大的网络，建造足够大的楼阁，这样上面说的问题就不足忧虑了。你有足够的算力来让机器机器学习网络图片，及用户上传的图片，这两种图片就不会竞争算力了。你算法的大脑足够大，因此你不需要担心被挤出阁楼。 

但是如果你没有足够大的神经网络(或者其他高度灵活的算法)，那你需要注意，让你的训练样本集与开发/测试样本集中数据分布相匹配了。 

如果你认为你的数据没啥益处，你应该把它们排除在计算之外。比如，假设你的开发/测试样本集中主要包含人、地方、地标、动物等无效图片，比如你有大量的历史文档扫描图片。 
![](http://www.insideai.cn/ngmly/1.37_doc.png)
这些文档图片中没有包含任何像猫的图片。他们跟开发/测试样本集的分布完全不同。将这些图片作为负例没有任何意义，因为神经网络从这部分数据中，学不到任何能应用在开发/测试样本集上的知识。将这些图片放入训练样本集，只会浪费计算资源和神经网络的表示能力。

# <span id='38'>38.是否要使用不一致的数据 ?</span>
假如你想让机器学会预测纽约的房价。即通过房屋面积(样本特征x),预测房屋售价(样本标签y)。纽约的房价非常高，假设你还有一个数据库，这个数据库中的数据为密歇根的底特律的房价，底特律的房价会低很多。你是否应该将底特律的房价数据，纳入到训练样本集中呢? 

提供相同的面积特征x,在两个城市房屋售价y表现出来的值有很大的差别。如果你只想预测纽约的房价，那么把底特律的房价数据放入训练集，会损害算法的性能。这时，最好不要把底特律的房价数据放入训练集中。 

为什么房价数据的处理方式跟之前猫咪数据的处理方式不同呢？这是因为，在猫咪图片数据中，给出一个图片x，就可以确切的预测出样本标签y，也就是图片中是否包含猫，这里图片x和样本标签y有一个确定的映射关系。因此可以把从网上下载的图片和手机APP用户上传的图片放到一起进行训练。这时，使用所有数据的好处大于的对应的坏处(消耗更多的计算资源)。与之相反，纽约的房价数据和底特律的房价数据是不一致的。给出相同的房屋面积x，房价y会因为城市的不同而不同。

# <span id='39'>39.区分不同数据的权重 ?</span>
假如从互联网上下载了200,000张图片，然后有获取了用户从手机APP上传的5,000张图片，这两种数据的比例是40:1。理论上，只要你训练最够大的神经网络，然后在205，000张图片上训练足够长的时间，这样就可以让你的算法，在网络图片和手机上传的图片上，都能有很好的识别效果。 

但是在实践中，相对于手机上传的图片，网络图片要多出40倍，这意味着，相比用5,000图片训练模型，我们要多耗费40倍(或更多)的计算资源。如果你没有足够多的计算资源，则可以妥协一下，你可以给网络图片设置非常小的权重。 

比如，假设你要优化的目标是平方误差(对于分类任务来说，这个不是很好的选择，但是它非常简单，且易于解释)。因此我们的学习算法会做出如下优化： 
![](http://www.insideai.cn/ngmly/1.39_1.png)
公式中的第一项为5000张手机图片的损失和，第二项为网上200,000张图片的损失和。你可以通过额外的参数 𝛽来优化这个式子： 
![](http://www.insideai.cn/ngmly/1.39_2.png)
如果 𝛽被设置成1/40,你的算法会给5,000张手机图片和200,000张网络图片相同的权重。你可以依据算法在开发测试集上的表现，将 𝛽设置成其他值。 

通过让网络图片的权重降低，你不需要一个大型的网络，来让算法在两种数据类型上都表现很好。这种重新定义权重的方法，只有在额外添加的数据(网络图片)与开发/测试样本集数据分布存在很大差异时，或额外添加的数据比开发/测试集中的数据多很多的时候，才会用到。


# <span id='40'>40.从训练数据集泛化到开发数据集 ?</span>
假如你正在配置一个模型，这个模型中训练样本集中的数据分布与开发/测试样本不同。   

比如说，训练样本集中包含网络图片和手机上传图片，而测试/开发样本集中只包含手机上传的图片。然后，此时算法表现很差，它在开发/测试样本集上的错误率远高于你的期望。造成这种现象的原因可能有以下几个： 

1. 它在训练样本集上表现的也很差。这时模型在训练样本集中，有高偏差的问题。 

2. 他可以在训练样本集上表现的很好。但是算法不能泛化到与训练样本集分布相同，但是之前没见过的数据上，这是高方差的问题。 

3. 模型可以很好的泛化到相同分布的数据上，但是在不同分布的开发/测试样本集上，表现的很差。我们称这个问题叫做数据不匹配，这是因为训练样本集与开发/测试样本集匹配度很差。 

比如，在人类可以取得近乎完美的猫的图片识别任务，你的算法表现如下：   
1. 在训练样本集上的错误率为1% 
2. 在与训练数据分布相同，但是之前没用过的数据上，算法的错误率为1.5% 
3. 在开发数据集上的错误率为10% 

在这个例子中，你可以很明显的看出数据不匹配问题。为了处理这个问题，你需要让训练数据更加接近开发/测试数据。我们之后将会讨论与之相关的技术。   

为了诊断出算法出现了上面三个问题中的哪一个，你需要再使用一个数据集。因此，相比把所有的训练数据都用作训练，你应该把训练数据分成两部分：一部分用作算法的训练，另一部分称作训练开发集，这部分数据不参与训练。 

你现在有四个数据子集：   

1. 训练数据集：算法用于学习训练的数据(比如网络图片+用户上传的图片)。这部分数据与我们关心数据(开发/测试样本集)的分布不同。 

2. 训练开发集：这部分数据与训练数据集的分布相同(比如网络图片+用户上传图片)，他通常会比训练样本集小，因为它只要能评估和追踪算法的进度就可以了。 

3. 开发数据集：这部分数据与测试数据集分布相同，反映了我们真是关心的数据分布(比如用户上传的图片数据分布) 

4. 测试数据集：这部分数据与开发数据集分布相同。 

拥有这四个数据集，你可以完成以下评估： 
1. 通过训练样本集评估训练错误率 
2. 通过训练开发集，评估算法在同一分布数据上的泛化能力 
3. 通过开发/测试样本集，评估算法在你真正关心数据上的表现 

5-7章所说的开发测试大小的选择方法，也同样适用于训练开发集大小的选择。






# <span id='41'>41.区分偏差、方差和数据不匹配问题 ?</span>
假设⼈类可以在猫咪识别的任务中达到近乎完美的性能（约等于0%的错误率），那么，最优的错误率就约为0%。假如系统性能表现是：   
1. 训练集上的错误率为1% 
2. 训练开发集上的错误率为5% 
3. 开发集上的错误率为5% 

这种情况说明了什么？你可以看到，方差很高。之前讲到的降低方差的方法可以帮助你处理当前的情况。 现在，再假设你的算法性能如下： 
1. 训练集上的错误率为10% 
2. 训练开发集上的错误率为11% 
3. 开发集上的错误率为12% 

由此可以看出，你的算法在训练集上的可避免偏差很高，也就是说算法在训练集上表现很差。这时偏差处理相关的方法可以对你有帮助。 

在前面的两个例子中，算法产生了很高的可避免偏差或是很高的方差。算法还有可能同时面对，高可避免偏差、高方差和数据不匹配当中的多个问题。例如： 
1. 训练集上的错误率为10% 
2. 训练开发集上的错误率为11% 
3. 开发集上的错误率为20% 

算法的可避免偏差很高，并且也有数据不匹配的问题。但是，算法在训练集上的方差并不高。为了更清晰理解不同类型的错误之间的相关性，我们把它们写在表格中： 
![](http://www.insideai.cn/ngmly/1.41_1.png)
继续之前的猫咪图像检测器例子，你可以看到，在X轴上有两种不同的数据分布。在Y轴上，有3种不同类型的错误率：人类错误率、算法在已经训练过的样本上的错误率、算法在从未训练过的样本上的错误率。我们可以在表格中第一列填上这三种不同类型的错误率。 

如果你愿意,你也可以补充表格中的后两列的内容：你可以找些人对手机上的猫咪图片进行标记，并且评估他们的错误，然后来填写右上角的表格。 

你可以按照分布B选取一些手机上的猫咪图片，并且把它们放在训练集中，让神经网络来学习。然后你来评估在这个数据子集上，学习模型的错误率。 

填写表中的这些条目，可以给你直观的感受到算法在两种不同的数据分布（分布A和分布B）上的表现。 

通过了解你的算法所面临的问题，你可以更好的决策下一步应该专注于减少偏差，方差还是减少数据不匹配的问题。

# <span id='42'>42.处理数据不匹配问题 ?</span>
假设你开发了一款语音识别系统，在训练集和训练开发集上表现很好。然而，系统在开发集上表现很差：这是一个数据不匹配的问题。你会怎么做？ 

我的建议： 
1. 了解训练集和开发集上数据分布的不同点。 
2. 找到更多和你开发集分布一致的数据。 

举例来说，假设你在语音识别的开发集上进行误差分析：你手动检查了100个样本，然后试着分析算法出错的原因。你发现你的算法表现不好，是因为开发集上的大部分的语音片段是在车里录制的，而训练集上的语音样本是在背景安静的环境下录制的。汽车发动机还有马路上的噪音严重影响了你的语音系统的识别性能。这种情况下，你可能需要试着获取更多在车上录制的语音片段，并加入训练集。误差分析的目的是了解训练集和开发集之间主要的区别，弄清楚导致数据不匹配的主要问题。 

如果你的训练集和训练开发集包括那些在车上录制的音频样本，你也应该在这部分的数据子集上双重地检查你的系统性能。如果在训练集上，算法对车内录制的音频样本数据表现很好，但是在训练开发集上，对车内录制的样本表现很差，这就验证了一个假设，获取更多在车上录制的音频样本会很有帮助。这就是为什么我们会在之前的章节中讨论，训练集上要包含一些样本数据，和开发/测试集数据的数据分布相同。这样做还可以让你对比在训练集以及开发/测试集上，关于车内音频数据上性能的不同。 

不幸的是，这个过程是无法保证的。比如，如果你怎么都找不到可以开发集数据相匹配的测试数据，那么，你想要提升性能就没有十分清晰有效的方法。

# <span id='43'>43.人工数据合成 ?</span>
你的语音识别系统需要更多听起来在车里录制的语音数据。相对于收集那些在开车的时候录制的音频，有一个更简单的方法获取这些数据：通过人工合成数据。 

假设你已经获得足够的汽车/马路噪音的音频片段。你可以从多个网站上下载这些数据。如果你已经有很多安静环境下录制的训练样本。你可以选出其中一个音频片段，然后给它加上背景噪音，这样你就获得了一个听起来好像在嘈杂的车上录制音频。 

用这种方法，你可以合成大量的数据，就好像是收集了大量在车里录制的语音。 

更广泛的说，在一些应用场景中，人工数据合成让你可以创造一个庞大的数据集来充分的匹配开发样本集。用猫咪检测器作为第二个例子。你会意识到开发样本集中很多会有运动模糊，这是因为手机用户在照相的时候都会出现轻微的抖动。 

你可以从网上找到没有模糊的图片组成训练集，然后加上一些模拟的运动模糊。 

要记得人工数据合成也有它的挑战：有时候，合成的数据在人看来很真实，但是在机器看来就没有那么真实了。例如，假设你有1000个小时的语音训练数据，但是只有1个小时的汽车噪音数据。在合成数据的时候，如果你在训练样本集中1000个小时的原始数据中，总是重复的使用这1个小时的汽车噪音，最后你就得到一个有很多重复噪音的合成数据集。对于人来说可能并不能听出有失真的情况，毕竟所有的汽车噪音对我们来说都没有太大的区别，但是机器学习算法却可能因为这重复的1个小时的汽车噪音变得过拟合。那么，算法就很难泛化到其他新的有背景噪音的音频片段上，当汽车噪音和原来不同的时候，算法就会遇到识别的麻烦。 

另一种相似的情况，假设你有1000个小时的汽车噪音，但是这些音频只来自10种不同的汽车。这种情况下，算法很可能会对这10种汽车过拟合，可能在处理其他类型车子的声音时性能表现的很差。不幸的是，这些问题很难会被注意到。 

再举一个例子，假设你正在建造一计算机视觉系统来识别车子。假设你有一个伙伴在视频游戏公司，有几种车子图像模型。为了训练你的算法，你会用这些模型来生成车子的合成图像。即使这些合成的图片看起来非常真实，这个方法可能还是不能很好的工作。所有视频游戏的场景中可能只用到20种左右的车子的设计，如果你在玩游戏，你很可能根本不会注意到你看到的都是重复的车子，很多只是车子颜色不同。也就是说，这些车子在你看来都很真实。但是和马路上真实在开动的车子相比，也就是在开发/测试集中的样本相比，这些20种型号的车子只是世界上车子类型里很小很小的一部分。如果你用20种型号的车子来合成了100000个测试样本，你的系统会对这20种车子过拟合，系统会无法泛化，也就无法在开发/测试样本集中识别出其他车子的设计。 

在合成数据的时候，要有意识的提醒自己是不是真的合成出了有代表性的样本。尽量避免使用那些能够被学习算法识别出人工合成的痕迹，比如所有数据都是从20种车子合成而来，或是所有的音频样本都是仅由1个小时的汽车噪音而来。这个建议实际上很难遵守。当合成数据时，我们的团队有时会在产生数据之前花上好几个星期在一些重要的细节上，来保证合成的数据可以足够接近真实的数据分布。如果你想正确的获知这些细节，你必须提前就接触非常大量的训练样本集。

# <span id='44'>44.优化验证测试</span>
假设你正在构建一个语音识别系统，该系统通过输入一个音频片段 $A​$ ，并为每一个可能的输出句子 $S​$ 计算得分 $\text{Score}_A(S)​$。例如，你可以试着估计 $ \text{Score}_A(S)=P(S|A)​ $ ，表示句子 $S​$ 是正确输出的转录的概率，其中 $A​$ 是给定的输入音频。

给定某种方法能够计算 $ \text {Score}_A(S) $ 后，你仍然需要找到一个英文句子 $S$ 来使之最大化：

$$ \text{Output} = \arg \max_s \text{Score}_A(S) $$

要如何去计算上面的 $\arg \max$ 呢？假设在英文中共有 5000 个词汇，对于长度为 $N$ 的句子，则有 $50000^N$ 种搭配，多到根本无法一一列举。因此，你需要使用一种近似搜索算法，努力去找到能够优化（最大化）$ \text{Score}_A(S)$ 的那个 $S$ . 有一种叫做 “定向搜索” 的搜索算法，在搜索过程中仅保留最优的 $K$ 个候选项（在本章中你并不需要了解该算法的细节）。类似这样的算法并不足以保证能够找到满足条件的 $S$ 来最大化 $ \text{Score}_A(S)$ .

假设有一个音频片段记录着某人说的：“我爱机器学习。”但你的系统输出的却是不正确的 “我爱机器人。”，它没能够输出正确的转录。造成该误差的可能原因有两种：

搜索算法存在问题。 近似搜索算法没能够找到最大化 $ \text{Score}_A(S)$ 的那个 $S$ .
目标（得分函数）存在问题。 我们对 $\text{Score}_A (S)=P(S|A)$ 的估计并不准确。尤其在此例中，我们的得分函数没能辨认出 “我爱机器学习” 是正确的转录。
对应不同的失败原因，你需要优先考虑的工作方向也将很不一样。如果原因 1 导致了问题，你应该改进搜索算法。如果原因 2 导致了问题，你应该在评估学习算法的函数 $ \text{Score}_A(S)$ 上面多花些心思。

面对这种情况，一些研究人员将决定研究搜索算法；其他人则努力去找到更好的 $ \text{Score}_A(S)$ . 但是，除非你知道其中哪一个是造成误差的潜在原因，否则你的努力可能会被浪费掉。怎么样才能更系统地决定要做什么呢？

让我们用 $S_{out}$ 表示实际的输出 “我爱机器人”，用 $S^*$ 表示正确的输出 “我爱机器学习” 。为了搞清楚上面的 1 或 2 是否存在问题，你可以执行 优化验证测试（Optimization Verification test）： 首先计算 $S_{out}$ 和 $S^*$ ，接着比较他们的大小。有两种可能：

情况1：$ \text{Score}_A {(S^*)} \gt \text{Score}_A {(S_{out})} $

在这种情况下，你的学习算法正确地给了 $S^*$ 一个比 $S_{out}$ 更高的分数。尽管如此，我们的近似搜索算法选择了 $S_{out}$ 而不是 $S^*$ . 则表示你的近似搜索算法没能够找到最大化 $ \text{Score}_A(S)$ 的那个 $S$ . 此时优化验证测试告诉你，搜索算法存在着问题，应该花时间研究。例如，你可以尝试增加定向搜索的搜索宽度。

情况2：$ \text{Score}_A (S^*) \leq \text{Score}_A (S_{out}) $

在这种情况下，计算 $\text{Score}_A (.)$ 的方式是错误的：它没有给正确的输出 $S^*$ 比实际输出 $S_{out}$ 一个相同或更高的分数。优化验证测试告诉你，目标（得分函数）存在问题。因此，你应该专注于改进你的算法对不同的句子 $S$ 学习或近似出得分 $\text{Score}_A (S)$ 的方式。

我们上面的讨论集中于某个单一的样本 $A$ 上，想要在实践中运用优化验证测试，你需要在开发集中检测这些误差样本。对于每一个误差样本，你都需要测试是否有 $\text{Score}_A (S^*) \gt \text{Score}_A (S)_{out}$ . 开发集中所有满足该不等式的样本都将被标记为优化算法自身所造成的误差，而满足不等式 $\text{Score}_A (S^*) \leq \text{Score}_A (S)_{out}$ 的样本将被记为是计算得分 $\text{Score}_A (.)$ 造成的误差。

假设你最终发现 95% 的误差是得分函数 $\text{Score}_A (.)$ 造成的，而仅有 5% 的误差是由优化算法造成的。现在你应该知道了，无论你如何改进你的优化程序， 实际上也只会消除误差中的 5% 左右。因此，你应该专注于改进你的得分函数 $\text{Score}_A (.)$ .

